# ML Model Drift Detection & Performance Monitoring System

## ğŸ” Overview
This project demonstrates how to monitor deployed Machine Learning models by detecting **data drift** and **prediction drift** on new incoming datasets.

The system compares new data distributions against baseline training statistics and flags potential risks to model reliability.

---

## âš™ï¸ Tech Stack
- Python
- Scikit-learn
- Pandas
- NumPy
- Streamlit
- Joblib

---

## ğŸ§  Key Concepts Implemented
- Feature Drift Detection (mean shift vs baseline standard deviation)
- Prediction Drift Detection (positive-rate distribution change)
- ML Pipeline with ColumnTransformer
- Baseline statistics persistence
- Model health status classification (STABLE / MONITOR / HIGH RISK)

---

## ğŸš€ How It Works
1. Train the ML model and save:
   - Model pipeline
   - Baseline feature statistics
   - Baseline prediction distribution
2. Upload a new dataset via Streamlit
3. System computes:
   - Feature-level drift
   - Prediction drift
4. Model health status is reported as:
   - **STABLE**
   - **MONITOR**
   - **HIGH RISK**

---

## â–¶ï¸ Run the Application
```bash
pip install -r requirements.txt
streamlit run app.py
```

## Project Structure
â”œâ”€â”€ app.py                 # Streamlit UI and workflow control
â”œâ”€â”€ model_utils.py         # Model & baseline loaders
â”œâ”€â”€ drift_utils.py         # Feature & prediction drift logic
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ model_pipeline.pkl
â”‚   â”œâ”€â”€ baseline_stats.pkl
â”‚   â””â”€â”€ baseline_positive_rate.pkl
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
